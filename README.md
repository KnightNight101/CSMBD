# CSMBD
## Abstract
This report presents a detailed account of the design, implementation, and evaluation of a multithreaded MapReduce pipeline developed in Python. The goal of this project was to process structured flight data to extract and rank passenger flight frequencies through parallelized mapping and reducing stages. The system is evaluated not only for its technical accuracy but also for the design decisions made, obstacles overcome, and conceptual alignment with MapReduce principles. Git was used for version control and link can be found above.
## Introduction
In recent decades, the proliferation of data in has necessitated the development of scalable and efficient data processing frameworks. Among these, the MapReduce paradigm has emerged as a dominant model, particularly suited for handling vast datasets in distributed environments. Although originally conceived for massive infrastructures like Google's internal clusters, the core ideas behind MapReduce can be effectively illustrated in smaller-scale implementations. This project aims to reproduce the MapReduce workflow using Python's threading module. By doing so, it provides an accessible yet powerful means of demonstrating parallel data processing principles within a constrained computational environment.
The focus of the task is to analyse passenger flight data to determine which passengers have taken the most flights. The mapper stage identifies and emits each occurrence of a passenger, while the reducer consolidates these occurrences to compute total flight counts. 
## System Architecture and Development Process
The system was created with modularity and extensibility as guiding principles. It is divided into several components, each housed in its own Python file under a designated mapreduce package. This approach ensures that each component can be independently tested and potentially reused in future projects. At the foundation of the system are two abstract base classes: BaseMapper and BaseReducer. These define the interface and behaviour expected from any mapper or reducer subclass, enforcing a contract that enhances code clarity and maintainability.
The implementation for this task includes PassengerFlightMapper, a class that processes chunks of CSV lines and emits tuples in the form of (PassengerID, 1). This abstraction mirrors the core concept of the MapReduce 'map' step, where input records are converted into key-value pairs. The reducer counterpart, PassengerFlightReducer, is responsible for aggregating all values associated with a given key—in this case, summing the occurrences of each passenger identifier to produce a final count. The central orchestrator of the system is the MapReduceJob class. It is responsible for dividing the input data into equal-sized chunks, assigning each to a separate thread running a mapper instance, and then collecting and grouping the outputs in a manner analogous to the 'shuffle and sort' phase in traditional MapReduce systems. Once grouped, the data is passed to reducer threads, which perform the aggregation and contribute to the final sorted output. The orchestrator also offers the flexibility to limit the output to the top N results, a feature implemented for user convenience and evaluation.
In main.py, the system is actually activated. This script handles file I/O, initiates the MapReduce job, and manages the post-processing of results, including writing the outcome to a text file. The decision to separate this control logic from the MapReduce engine reflects best practices in software engineering, where the application logic is decoupled from the business logic.
## Decision-Making and Contextual Rationale
The AComp_Passenger_data_no_error_DateTime.csv dataset provided clean and relevant fields—namely, passenger ID, timestamp, origin and destination airports, and flight number. The use of multithreading was not merely an exercise in concurrency but a deliberate simulation of the distributed nature of real-world MapReduce platforms such as Hadoop. Python’s Global Interpreter Lock (GIL) imposes certain constraints on concurrent execution, but for I/O-bound and structurally simple tasks like CSV parsing, multithreading provides a meaningful performance benefit. More importantly, it introduced design challenges, especially concerning shared resources, that necessitated careful management using synchronization primitives such as Lock.
Modularity was another pillar of the system's architecture. Inspired by principles from domain-driven design and object-oriented software engineering, each component was given a clear responsibility. This made it easier to debug, document, and explain each module’s purpose. For instance, the abstract base classes not only enforced consistency among mappers and reducers but also provided a reusable scaffold for any future expansion of analytical scope.
In terms of language features, Python's typing module was employed to provide static type hints. This was both a stylistic and functional decision: while Python is dynamically typed, type hints improve IDE support, enable static analysis, and document developer intent. Early versions of the code incorrectly annotated optional parameters, such as using int = None rather than the correct Optional[int]. This was caught and rectified during linting.
Another subtle yet critical issue arose from the use of lambda functions within threading constructs. When using lambdas in a loop without binding the current variable to the function’s scope, Python captures the loop variable by reference, not by value. As a result, all threads were erroneously using the final item in the loop. This bug was identified through unexpected output patterns and fixed by binding default arguments within the lambda function (e.g., lambda m=mapper: ...).
The final touch was to provide a persistent textual summary of the output, written to a .txt file. This was implemented using a simple file I/O block that appended the top 10 results to passenger_flight_counts.txt. While not a sophisticated reporting system, it serves as a reproducible output format suitable for inclusion in appendices or further analytical processing.
## Challenges Encountered and Solutions Devised
Several challenges emerged during the development lifecycle, some anticipated and others not. One of the more persistent issues was related to file path management. Because the execution environment occasionally shifted between different directories, relative path mismatches led to runtime errors. This was mitigated by standardizing all path constructions through Python’s os.path.join() function, ensuring cross-platform compatibility and reducing human error.
A structural challenge came in the form of method duplication. During the iterative development process, multiple versions of the run() method coexisted within the MapReduceJob class. This not only led to confusion but also caused hard-to-diagnose bugs. Upon review, the redundant version was removed, and docstrings were standardized to improve readability and maintainability.
Perhaps the most conceptual challenge was simulating the shuffle and sort phase of MapReduce. While Python's dictionaries provide an easy way to group values by key, ensuring that this was done safely across multiple threads required both a correct design and careful implementation. The use of defaultdict(list) provided the necessary scaffolding, while locking mechanisms ensured that concurrent writes did not corrupt the data.
## Theoretical and Practical Understanding of MapReduce
In theory, MapReduce is a model for parallel computation that decomposes problems into mappable and reducible subcomponents. In practice, it serves as the backbone for numerous distributed systems, processing everything from search indexes to machine learning pipelines. By mirroring this architecture in a simplified, thread-based model, this project bridges theoretical foundations with practical programming skills.
The PassengerFlightMapper and PassengerFlightReducer were designed to follow this paradigm explicitly. Each mapper reads and processes a chunk of the dataset, emitting key-value pairs. These pairs are then grouped by key in the shuffle phase. The grouped data is passed to reducers, which aggregate values into a final result. This pipeline is not only functional but scalable in its design, ready to be extended to different data domains or deployed in more robust parallel environments.
Importantly, the system encourages extensibility. For example, new mappers and reducers could be introduced to analyse route frequencies, detect frequent flyer patterns, or even study seasonal trends. Thanks to the use of base classes and consistent interfaces, these enhancements would require minimal reconfiguration of the core engine.
## Conclusion and Reflections
This project succeeded in developing a robust and scalable framework for processing structured flight data using a MapReduce-inspired architecture. Despite working within the constraints of a local Python environment, the system managed to capture the essence of distributed computing through multithreading, modular design, and disciplined software engineering practices.
Several important lessons were learned throughout development. One was the value of clear architectural boundaries between components. Another was the importance of understanding the language’s subtleties—in particular, how Python handles variable scope in closures and type annotations. The use of version control also enables the software to be tracked, with back up versions saved online on Github. In addition to this, the workflow can be ported to larger teams and larger processes, using forks to separate versions that are in development to those that are deployed.
Looking forward, the project could benefit from transitioning to multiprocessing to bypass the limitations imposed by the GIL. Alternatively, porting the code to a distributed system like Apache Spark could enable it to handle truly large datasets. Either path would build upon the architectural foundation laid by this work.
In summary, the project not only met the technical requirements but also served as a rich learning experience in the design and implementation of scalable data systems. It stands as a solid foundation for further research and development in the domain of big data analytics.
